{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torchtext\n",
    "\n",
    "from data import dataset\n",
    "import tweet_utils as tu\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.csv successfully shuffled!\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makskonevych/Documents/Python/ML/KaggleCompetitions/DisasterTweets/data/dataset.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._data['text'] = self._data['text'].apply(tweet_prep_fn)\n",
      "/Users/makskonevych/Documents/Python/ML/KaggleCompetitions/DisasterTweets/data/dataset.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._data['text'] = self._data['text'].apply(tweet_prep_fn)\n"
     ]
    }
   ],
   "source": [
    "! python3 data/_shuffle.py data/\n",
    "train_dataset = dataset.DisasterData('train', path='data/', tweet_prep_fn=tu.process_tweet)\n",
    "valid_dataset = dataset.DisasterData('valid', path='data/', tweet_prep_fn=tu.process_tweet)\n",
    "test_dataset = dataset.DisasterData('test', path='data/', tweet_prep_fn=tu.process_tweet)\n",
    "all_dataset = dataset.DisasterData('all', path='data/', tweet_prep_fn=tu.process_tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "EMBEDDING_N = 200\n",
    "vec = torchtext.vocab.GloVe(name='twitter.27B', dim=EMBEDDING_N)\n",
    "UNK_T = torch.randn(EMBEDDING_N)\n",
    "EOS_T = torch.randn(EMBEDDING_N)\n",
    "PAD_T = [0.] * EMBEDDING_N\n",
    "vec.unk_init = lambda x: UNK_T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(vec['thereisnotimeforfussingandfighting'], UNK_T)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "((7381,\n  'meek mill begging nicki minaj to let him obliterate ovofest nowplaying t.co t.co',\n  'Toronto',\n  'obliterate'),\n 0)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def text_to_vector(text, pad_n=0, add_eos=True, prep_fn=tu.process_tweet):\n",
    "    if prep_fn:\n",
    "        tokens = prep_fn(text, rm_weblinks=True).split()\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "\n",
    "    res = [vec[token] for token in tokens]\n",
    "    if add_eos:\n",
    "        res += [EOS_T]\n",
    "    if pad_n > 0:\n",
    "        res += [PAD_T] * pad_n\n",
    "\n",
    "    return [[float(v) for v in b] for b in res]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    ids = []\n",
    "    X = []\n",
    "    targets = []\n",
    "    max_len = 0\n",
    "\n",
    "    for ((id, text, location, keyword), target) in batch:\n",
    "        X.append(text_to_vector(text, prep_fn=None))\n",
    "        max_len = max(len(X[-1]), max_len)\n",
    "\n",
    "        ids.append(id)\n",
    "        targets.append([target])\n",
    "\n",
    "    for v in X:\n",
    "        for t in range(len(v), max_len):\n",
    "            v.append(PAD_T)\n",
    "\n",
    "\n",
    "    return torch.tensor(ids), torch.tensor(X), torch.tensor(targets, dtype=torch.float) if targets[0][0] is not None else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_dataloader = utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "all_dataloader = utils.data.DataLoader(all_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "valid_dataloader = utils.data.DataLoader(valid_dataset, batch_size=32, collate_fn=collate)\n",
    "test_dataloader = utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_d, hidden_d, layers_n=1, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.Lstm1 = nn.LSTM(input_size=input_d, hidden_size=hidden_d, num_layers=layers_n//2, batch_first=True, bidirectional=bidirectional)\n",
    "        self.Lstm2 = nn.LSTM(input_size=hidden_d * (2 if bidirectional else 1), hidden_size=hidden_d, num_layers=layers_n - layers_n//2, batch_first=True, bidirectional=bidirectional)\n",
    "        self.Fc = nn.Linear(hidden_d * (2 if bidirectional else 1), 1)\n",
    "        self.Dropout1 = nn.Dropout(0.7)\n",
    "        self.Dropout2 = nn.Dropout(0.5)\n",
    "        self.Dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            x = x[:,None,:]\n",
    "\n",
    "        x = self.Dropout1(x)\n",
    "        x, _ = self.Lstm1(x)\n",
    "\n",
    "        x = self.Dropout2(x)\n",
    "        x, _ = self.Lstm2(x)\n",
    "        x = x[:,-1,:]     # taking the last output for each sequence\n",
    "\n",
    "        x = self.Dropout3(x)\n",
    "        x = self.Fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model1 = Model(EMBEDDING_N, 50, layers_n=4, bidirectional=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (Lstm1): LSTM(200, 50, num_layers=2, batch_first=True, bidirectional=True)\n  (Lstm2): LSTM(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n  (Fc): Linear(in_features=100, out_features=1, bias=True)\n  (Dropout1): Dropout(p=0.7, inplace=False)\n  (Dropout2): Dropout(p=0.5, inplace=False)\n  (Dropout3): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters number: 6402\n"
     ]
    }
   ],
   "source": [
    "n_param = 0\n",
    "for param in model1.parameters():\n",
    "    n_param += len(param)\n",
    "\n",
    "print('Parameters number:', n_param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, x, y in dataloader:\n",
    "            y_hat = torch.sigmoid(model.forward(x)).round()\n",
    "            accurate += float((y_hat == y).sum())\n",
    "            total += float(x.shape[0])\n",
    "\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def train(epochs, model, optimizer, valid_dataloader, train_dataloader, loss_fn=nn.BCELoss(), display_epoch=0, backup_dir=None, threshold=.82):\n",
    "    print(\"Initial validation accuracy:\", get_accuracy(model, valid_dataloader))\n",
    "\n",
    "    max_v_acc = 0.\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        j = 0\n",
    "        epoch_loss = 0\n",
    "        batches = len(train_dataloader)\n",
    "\n",
    "        accurate = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for b_i, (id, x, y) in enumerate(train_dataloader):\n",
    "            y_hat = torch.sigmoid(model.forward(x))\n",
    "            loss = loss_fn(y_hat, y)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                accurate += float((y_hat.round() == y).sum())\n",
    "                total += float(x.shape[0])\n",
    "\n",
    "            j += 1\n",
    "            epoch_loss += float(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if b_i % (batches // 12) == 0:\n",
    "                print('-', end='')\n",
    "\n",
    "        cur_v_acc = get_accuracy(model, valid_dataloader)\n",
    "        if backup_dir and cur_v_acc > max_v_acc and cur_v_acc > threshold:\n",
    "            torch.save(model.state_dict(), backup_dir + \"lstm%.4f.pt\" % cur_v_acc)\n",
    "            print(' *', end='')\n",
    "        else:\n",
    "            print(' -', end='')\n",
    "\n",
    "        max_v_acc = max(max_v_acc, cur_v_acc)\n",
    "        print(' EPOCH %03d' % int(i + 1 + display_epoch),\n",
    "              '| TLoss: %.4f' % round(epoch_loss / j, 4),\n",
    "              '| TAccuracy: %.4f' % round(accurate / total, 4),\n",
    "              '| VAccuracy: %.4f' % cur_v_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy: 0.4225721784776903\n",
      "------------- - EPOCH 001 | TLoss: 0.6893 | TAccuracy: 0.5370 | VAccuracy: 0.5774\n",
      "------------- - EPOCH 002 | TLoss: 0.6820 | TAccuracy: 0.5696 | VAccuracy: 0.5774\n",
      "------------- - EPOCH 003 | TLoss: 0.6332 | TAccuracy: 0.6244 | VAccuracy: 0.7310\n",
      "------------- - EPOCH 004 | TLoss: 0.5730 | TAccuracy: 0.7234 | VAccuracy: 0.7782\n",
      "------------- - EPOCH 005 | TLoss: 0.5383 | TAccuracy: 0.7522 | VAccuracy: 0.8005\n",
      "------------- - EPOCH 006 | TLoss: 0.5188 | TAccuracy: 0.7593 | VAccuracy: 0.7940\n",
      "------------- - EPOCH 007 | TLoss: 0.5143 | TAccuracy: 0.7643 | VAccuracy: 0.8058\n",
      "------------- - EPOCH 008 | TLoss: 0.5039 | TAccuracy: 0.7724 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 009 | TLoss: 0.5017 | TAccuracy: 0.7682 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 010 | TLoss: 0.4971 | TAccuracy: 0.7698 | VAccuracy: 0.8123\n",
      "------------- - EPOCH 011 | TLoss: 0.4913 | TAccuracy: 0.7733 | VAccuracy: 0.8097\n",
      "------------- - EPOCH 012 | TLoss: 0.4905 | TAccuracy: 0.7751 | VAccuracy: 0.8005\n",
      "------------- - EPOCH 013 | TLoss: 0.4859 | TAccuracy: 0.7726 | VAccuracy: 0.8031\n",
      "------------- - EPOCH 014 | TLoss: 0.4840 | TAccuracy: 0.7765 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 015 | TLoss: 0.4776 | TAccuracy: 0.7840 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 016 | TLoss: 0.4787 | TAccuracy: 0.7799 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 017 | TLoss: 0.4767 | TAccuracy: 0.7783 | VAccuracy: 0.8071\n",
      "------------- - EPOCH 018 | TLoss: 0.4784 | TAccuracy: 0.7802 | VAccuracy: 0.8018\n",
      "------------- - EPOCH 019 | TLoss: 0.4729 | TAccuracy: 0.7824 | VAccuracy: 0.8005\n",
      "------------- * EPOCH 020 | TLoss: 0.4706 | TAccuracy: 0.7830 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 021 | TLoss: 0.4724 | TAccuracy: 0.7805 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 022 | TLoss: 0.4719 | TAccuracy: 0.7859 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 023 | TLoss: 0.4672 | TAccuracy: 0.7828 | VAccuracy: 0.8123\n",
      "------------- - EPOCH 024 | TLoss: 0.4742 | TAccuracy: 0.7848 | VAccuracy: 0.8189\n",
      "------------- * EPOCH 025 | TLoss: 0.4705 | TAccuracy: 0.7805 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 026 | TLoss: 0.4693 | TAccuracy: 0.7843 | VAccuracy: 0.8150\n",
      "------------- - EPOCH 027 | TLoss: 0.4658 | TAccuracy: 0.7838 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 028 | TLoss: 0.4645 | TAccuracy: 0.7838 | VAccuracy: 0.8123\n",
      "------------- - EPOCH 029 | TLoss: 0.4717 | TAccuracy: 0.7857 | VAccuracy: 0.8189\n",
      "------------- * EPOCH 030 | TLoss: 0.4681 | TAccuracy: 0.7805 | VAccuracy: 0.8228\n",
      "------------- - EPOCH 031 | TLoss: 0.4647 | TAccuracy: 0.7856 | VAccuracy: 0.8123\n",
      "------------- - EPOCH 032 | TLoss: 0.4611 | TAccuracy: 0.7872 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 033 | TLoss: 0.4664 | TAccuracy: 0.7898 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 034 | TLoss: 0.4670 | TAccuracy: 0.7848 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 035 | TLoss: 0.4513 | TAccuracy: 0.7907 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 036 | TLoss: 0.4636 | TAccuracy: 0.7862 | VAccuracy: 0.8163\n",
      "------------- - EPOCH 037 | TLoss: 0.4569 | TAccuracy: 0.7892 | VAccuracy: 0.8058\n",
      "------------- - EPOCH 038 | TLoss: 0.4528 | TAccuracy: 0.7948 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 039 | TLoss: 0.4623 | TAccuracy: 0.7875 | VAccuracy: 0.8228\n",
      "------------- - EPOCH 040 | TLoss: 0.4559 | TAccuracy: 0.7942 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 041 | TLoss: 0.4606 | TAccuracy: 0.7879 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 042 | TLoss: 0.4527 | TAccuracy: 0.7940 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 043 | TLoss: 0.4522 | TAccuracy: 0.7927 | VAccuracy: 0.8176\n",
      "------------- - EPOCH 044 | TLoss: 0.4504 | TAccuracy: 0.7990 | VAccuracy: 0.8097\n",
      "------------- - EPOCH 045 | TLoss: 0.4500 | TAccuracy: 0.8005 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 046 | TLoss: 0.4484 | TAccuracy: 0.7961 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 047 | TLoss: 0.4556 | TAccuracy: 0.7911 | VAccuracy: 0.8136\n",
      "------------- - EPOCH 048 | TLoss: 0.4472 | TAccuracy: 0.7929 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 049 | TLoss: 0.4502 | TAccuracy: 0.7926 | VAccuracy: 0.8215\n",
      "------------- * EPOCH 050 | TLoss: 0.4537 | TAccuracy: 0.7926 | VAccuracy: 0.8241\n"
     ]
    }
   ],
   "source": [
    "train(50, model1, optimizer, valid_dataloader, train_dataloader, display_epoch=0, backup_dir='lstm_backups/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy: 0.8228346456692913\n",
      "------------- * EPOCH 101 | TLoss: 0.4062 | TAccuracy: 0.8210 | VAccuracy: 0.8268\n",
      "------------- - EPOCH 102 | TLoss: 0.4026 | TAccuracy: 0.8203 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 103 | TLoss: 0.4120 | TAccuracy: 0.8165 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 104 | TLoss: 0.4075 | TAccuracy: 0.8199 | VAccuracy: 0.8228\n",
      "------------- - EPOCH 105 | TLoss: 0.4115 | TAccuracy: 0.8174 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 106 | TLoss: 0.4091 | TAccuracy: 0.8175 | VAccuracy: 0.8228\n",
      "------------- - EPOCH 107 | TLoss: 0.4056 | TAccuracy: 0.8175 | VAccuracy: 0.8163\n",
      "------------- - EPOCH 108 | TLoss: 0.4087 | TAccuracy: 0.8177 | VAccuracy: 0.8110\n",
      "------------- - EPOCH 109 | TLoss: 0.4046 | TAccuracy: 0.8260 | VAccuracy: 0.8071\n",
      "------------- - EPOCH 110 | TLoss: 0.4061 | TAccuracy: 0.8170 | VAccuracy: 0.7992\n",
      "------------- - EPOCH 111 | TLoss: 0.4029 | TAccuracy: 0.8209 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 112 | TLoss: 0.3913 | TAccuracy: 0.8304 | VAccuracy: 0.8058\n",
      "------------- - EPOCH 113 | TLoss: 0.4012 | TAccuracy: 0.8221 | VAccuracy: 0.8163\n",
      "------------- - EPOCH 114 | TLoss: 0.3980 | TAccuracy: 0.8222 | VAccuracy: 0.8031\n",
      "------------- - EPOCH 115 | TLoss: 0.4083 | TAccuracy: 0.8194 | VAccuracy: 0.8136\n",
      "------------- - EPOCH 116 | TLoss: 0.4043 | TAccuracy: 0.8228 | VAccuracy: 0.8176\n",
      "------------- - EPOCH 117 | TLoss: 0.3974 | TAccuracy: 0.8221 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 118 | TLoss: 0.4024 | TAccuracy: 0.8244 | VAccuracy: 0.8202\n",
      "------------- - EPOCH 119 | TLoss: 0.3998 | TAccuracy: 0.8206 | VAccuracy: 0.8228\n",
      "------------- - EPOCH 120 | TLoss: 0.4049 | TAccuracy: 0.8192 | VAccuracy: 0.8176\n",
      "------------- - EPOCH 121 | TLoss: 0.4004 | TAccuracy: 0.8193 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 122 | TLoss: 0.4009 | TAccuracy: 0.8189 | VAccuracy: 0.8215\n",
      "------------- - EPOCH 123 | TLoss: 0.3991 | TAccuracy: 0.8257 | VAccuracy: 0.8005\n",
      "------------- - EPOCH 124 | TLoss: 0.3938 | TAccuracy: 0.8232 | VAccuracy: 0.8031\n",
      "------------- * EPOCH 125 | TLoss: 0.3923 | TAccuracy: 0.8229 | VAccuracy: 0.8281\n",
      "------------- - EPOCH 126 | TLoss: 0.3971 | TAccuracy: 0.8183 | VAccuracy: 0.8189\n",
      "------------- - EPOCH 127 | TLoss: 0.3999 | TAccuracy: 0.8222 | VAccuracy: 0.8123\n",
      "------------- - EPOCH 128 | TLoss: 0.3870 | TAccuracy: 0.8321 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 129 | TLoss: 0.3913 | TAccuracy: 0.8257 | VAccuracy: 0.8202\n",
      "------------- * EPOCH 130 | TLoss: 0.3912 | TAccuracy: 0.8273 | VAccuracy: 0.8333\n",
      "------------- - EPOCH 131 | TLoss: 0.3883 | TAccuracy: 0.8308 | VAccuracy: 0.8018\n",
      "------------- - EPOCH 132 | TLoss: 0.3976 | TAccuracy: 0.8247 | VAccuracy: 0.7979\n",
      "------------- - EPOCH 133 | TLoss: 0.3959 | TAccuracy: 0.8282 | VAccuracy: 0.8071\n",
      "------------- - EPOCH 134 | TLoss: 0.3881 | TAccuracy: 0.8262 | VAccuracy: 0.8071\n",
      "------------- - EPOCH 135 | TLoss: 0.3929 | TAccuracy: 0.8243 | VAccuracy: 0.8163\n",
      "------------- - EPOCH 136 | TLoss: 0.3815 | TAccuracy: 0.8323 | VAccuracy: 0.8058\n",
      "------------- - EPOCH 137 | TLoss: 0.3878 | TAccuracy: 0.8251 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 138 | TLoss: 0.3841 | TAccuracy: 0.8330 | VAccuracy: 0.8084\n",
      "------------- - EPOCH 139 | TLoss: 0.3874 | TAccuracy: 0.8289 | VAccuracy: 0.7992\n",
      "------------- - EPOCH 140 | TLoss: 0.3908 | TAccuracy: 0.8272 | VAccuracy: 0.8084\n",
      "---"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisplay_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackup_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlstm_backups/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [15], line 16\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(epochs, model, optimizer, valid_dataloader, train_dataloader, loss_fn, display_epoch, backup_dir, threshold)\u001B[0m\n\u001B[1;32m     13\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     15\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m b_i, (\u001B[38;5;28mid\u001B[39m, x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataloader):\n\u001B[1;32m     17\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(model\u001B[38;5;241m.\u001B[39mforward(x))\n\u001B[1;32m     18\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(y_hat, y)\n",
      "File \u001B[0;32m~/Documents/Python/ML/MLEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Documents/Python/ML/MLEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/Python/ML/MLEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [7], line 8\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m      5\u001B[0m max_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ((\u001B[38;5;28mid\u001B[39m, text, location, keyword), target) \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[0;32m----> 8\u001B[0m     X\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtext_to_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprep_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[1;32m      9\u001B[0m     max_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), max_len)\n\u001B[1;32m     11\u001B[0m     ids\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mid\u001B[39m)\n",
      "Cell \u001B[0;32mIn [6], line 13\u001B[0m, in \u001B[0;36mtext_to_vector\u001B[0;34m(text, pad_n, add_eos, prep_fn)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pad_n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     11\u001B[0m     res \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [PAD_T] \u001B[38;5;241m*\u001B[39m pad_n\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [[\u001B[38;5;28mfloat\u001B[39m(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m b] \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m res]\n",
      "Cell \u001B[0;32mIn [6], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pad_n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     11\u001B[0m     res \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [PAD_T] \u001B[38;5;241m*\u001B[39m pad_n\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [[\u001B[38;5;28mfloat\u001B[39m(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m b] \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m res]\n",
      "Cell \u001B[0;32mIn [6], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pad_n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     11\u001B[0m     res \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [PAD_T] \u001B[38;5;241m*\u001B[39m pad_n\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [[\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m b] \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m res]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(50, model1, optimizer, valid_dataloader, train_dataloader, display_epoch=100, backup_dir='lstm_backups/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8241469816272966"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(model1, valid_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),\"lstm_backups/backup.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load(\"lstm_backups/lstm0.8333.pt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8307086614173228"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(model1, valid_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "for ids, x, _ in test_dataloader:\n",
    "    y_hat = torch.sigmoid(model1.forward(x)).round()\n",
    "    r = torch.concat([ids[:, None], y_hat], dim=1)\n",
    "    predictions_df = pd.concat([predictions_df, pd.DataFrame(r.detach().numpy(), columns=['id', 'target'])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "predictions_df.target = predictions_df.target.astype(int)\n",
    "predictions_df.id = predictions_df.id.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "       id  target\n0       0       1\n1       2       1\n2       3       1\n3       9       1\n4      11       1\n..    ...     ...\n26  10861       1\n27  10865       1\n28  10868       1\n29  10874       1\n30  10875       0\n\n[3263 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>10861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>10875</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "predictions_df.to_csv('predictions/LSTM.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load(\"lstm_backups/backup.pt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
